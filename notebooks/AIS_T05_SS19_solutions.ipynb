{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Analytical Information Systems*\n",
    "\n",
    "# Tutorial 5 - Predictive Modeling\n",
    "\n",
    "Matthias Griebel<br>\n",
    "Lehrstuhl für Wirtschaftsinformatik und Informationsmanagement\n",
    "\n",
    "SS 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "toc": true
   },
   "source": [
    "<h1>Agenda<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Follow-up-on-data-visualization:-Esquisse\" data-toc-modified-id=\"Follow-up-on-data-visualization:-Esquisse-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Follow up on data visualization: Esquisse</a></span></li><li><span><a href=\"#Data-Mining\" data-toc-modified-id=\"Data-Mining-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Mining</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training,-validation,-and-test-sets\" data-toc-modified-id=\"Training,-validation,-and-test-sets-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Training, validation, and test sets</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Data Preparation</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metrics-for-classification\" data-toc-modified-id=\"Metrics-for-classification-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Metrics for classification</a></span></li></ul></li><li><span><a href=\"#Up-to-you:--Titanic-Passenger-Survival-Classification\" data-toc-modified-id=\"Up-to-you:--Titanic-Passenger-Survival-Classification-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Up to you:  Titanic Passenger Survival Classification</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Follow up on data visualization: Esquisse\n",
    "\n",
    "The [Esquisse](https://github.com/dreamRs/esquisse) addin allows you to interactively explore your data by visualizing it with the ggplot2 package. It allows you to draw bar plots, curves, scatter plots, histograms, boxplot and sf objects, then export the graph or retrieve the code to reproduce the graph.\n",
    "\n",
    "<img src=\"https://github.com/dreamRs/esquisse/raw/master/man/figures/esquisse.gif\" style=\"heigth:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Esquisse works best in RStudio__\n",
    "\n",
    "1. Open RStudio\n",
    "    - For notebooks running on *binderhub*: Copy the first part of the URL from this notebook, e.g.: \n",
    "            `https://hub.gke.mybinder.org/user/matjesg-ais_2019-qgp8pz15/` \n",
    "    - and paste '/rstudio' at the end\n",
    "            `https://hub.gke.mybinder.org/user/matjesg-ais_2019-qgp8pz15/rstudio` \n",
    "- Start Esquisse\n",
    "    - Choose `Addins`-> `'ggplot2' builder`\n",
    "    - Or run `esquisse:::esquisser(viewer = \"pane\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__CRISP-DM__\n",
    "<img align=\"right\" src=\"http://statistik-dresden.de/wp-content/uploads/2012/04/CRISP-DM_Process_Diagram1.png\" style=\"width:50%\">\n",
    "\n",
    "Today, we will focus on \n",
    "- `Data Preparation`\n",
    "- `Modelling`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Tidymodels__\n",
    "\n",
    "Similar to its sister package `tidyverse`, it can be used to install and load `tidyverse` packages related to modeling and analysis. Currently, it installs and attaches `dplyr`,`ggplot2`, `purrr` as well as\n",
    "\n",
    "- `rsample`: Create and summarize different types of resampling objects (e.g. bootstrap, cross-validation)\n",
    "- `broom`: Summarizes key information about statistical objects in tidy tibbles\n",
    "- `recipes`: Define a recipe or blueprint that can be used to sequentially define the encodings and preprocessing of the data\n",
    "- `infer`: tidyverse-friendly statistical inference\n",
    "- `yardstick`: Tidy methods for measuring model performance\n",
    "\n",
    "\n",
    "The declared goal of the tidymodels metapackage is to provide a unified modelling synthax similar to scikit-learn in the python domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidymodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training, validation, and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__Information leaks__\n",
    "\n",
    "Every time you tune a hyperparameter of your model based on the model’s performance on the validation set, some information about the validation data leaks into the model.\n",
    "\n",
    "__How to avoid overfitting?__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Simple Hold-Out validation__\n",
    "\n",
    "- Set apart some fraction of your data as your test set \n",
    "- Train on the remaining data, and evaluate on the validation set (part of your the remaining data).\n",
    "- Once your model is ready for prime time, you test it one final time on the test data.\n",
    "\n",
    "<img align=\"center\" src=\"https://elitedatascience.com/wp-content/uploads/2017/06/Train-Test-Split-Diagram.jpg\" style=\"width:70%\">\n",
    "\n",
    "Source: [EliteDataScience](https://elitedatascience.com/model-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__K-fold cross-validation and K-fold stratified cross-validation__\n",
    "\n",
    "- Set apart some fraction of your data as your test set \n",
    "- Split the remaining data into `K` partitions of equal size. For each partition `i`, train a model on the remaining `K-1` partitions, and evaluate it on partition `i`.\n",
    "- Stratified cross-validation splits the data such that the proportions between classes are the same in each fold as they are in the whole dataset \n",
    "\n",
    "<img align=\"center\" src=\"images/05/cv_comparison.png\" style=\"width:90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Resampling with rsample__\n",
    "\n",
    "<img align=\"right\" src=\"https://tidymodels.github.io/rsample/rsample_hex_thumb.png\" style=\"width:20%\">\n",
    "\n",
    "`rsample` contains a set of functions that can create different types of resamples and corresponding classes for their analysis.\n",
    "\n",
    "- Traditional resampling techniques for estimating the sampling distribution of a statistic\n",
    "- Estimating model performance using a holdout set\n",
    "- Example: \n",
    "```\n",
    "data_split <- initial_split(data)\n",
    "train_data <- training(data_split)\n",
    "test_data <- testing(data_split)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Example: Credit data___\n",
    "\n",
    "The data set contains anonymized information about the credit status of bank customers.\n",
    "From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data(\"credit_data\")\n",
    "glimpse(credit_data)\n",
    "?credit_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Train-Test-Split___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split <- initial_split(credit_data, prop = 0.7)\n",
    "credit_train <- training(train_test_split)\n",
    "credit_test <- testing(train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this an appropriate way to split the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(credit_data$Status==\"bad\")/sum(credit_data$Status==\"good\")\n",
    "sum(credit_train$Status==\"bad\")/sum(credit_train$Status==\"good\")\n",
    "sum(credit_test$Status==\"bad\")/sum(credit_test$Status==\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Stratified Split___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(0)\n",
    "train_test_split <- initial_split(credit_data, prop = 0.7, strata = \"Status\")\n",
    "credit_train <- training(train_test_split)\n",
    "credit_test <- testing(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(credit_data$Status==\"bad\")/sum(credit_data$Status==\"good\")\n",
    "sum(credit_train$Status==\"bad\")/sum(credit_train$Status==\"good\")\n",
    "sum(credit_test$Status==\"bad\")/sum(credit_test$Status==\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "<img align=\"center\" src=\"images/05/data_prep.png\" style=\"width:80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Feature Engeneering__\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering.\" (Andrew Ng)\n",
    "\n",
    "\n",
    "Putting in plain vanilla variables into our model is often not the best idea (however, often a good start)\n",
    "- Think about creating new variables\n",
    "    - combine distance and time to create a speed variable\n",
    "- Group together similar concepts to reduce factor levels\n",
    "    - combine “lawyer“ and “judge” to “legal profession”\n",
    "- Identify thresholds through explorative data analysis and visualization\n",
    "- Reduce dimensionality of data through PCA or polynomial fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Data Preprocessing with recipes__\n",
    "\n",
    "<img align=\"right\" src=\"https://github.com/tidymodels/recipes/raw/master/man/figures/logo.png\" style=\"width:20%\">\n",
    "\n",
    "The idea of the recipes package is to define a recipe or blueprint that can be used to sequentially define the encodings and preprocessing of the data (i.e. feature engineering)\n",
    "\n",
    "1. __Recipe:__ Define roles (outcome, predictor, etc.) and the steps to be applied to a data set in order to get it ready for data analysis\n",
    "\n",
    "2. __Prepare:__ Estimate the required parameters from a training set that can be later applied to other data sets\n",
    "\n",
    "3. __Bake:__ Apply the recipes to the targeted dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Preprocessing___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some missing values in these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vapply(credit_train, function(x) mean(!is.na(x)), numeric(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than remove these, their values will be imputed.\n",
    "\n",
    "The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Initial Recipe___\n",
    "\n",
    "First, we will create a recipe object from the original data and then specify the processing steps.\n",
    "\n",
    "Recipes can be created manually by sequentially adding roles to variables in a data set.\n",
    "\n",
    "If the analysis only required outcomes and predictors, the easiest way to create the initial recipe is to use the standard formula method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_obj <- recipe(Status ~ ., data = credit_train)\n",
    "rec_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contained in the data argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Imputation of missing values___\n",
    "\n",
    "Here, K-nearest neighbor imputation will be used. This works for both numeric and non-numeric predictors and defaults K to five To do this, it selects all predictors then removes those that are numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed <- rec_obj %>%\n",
    "        step_knnimpute(all_predictors()) \n",
    "imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Dummy variables___\n",
    "\n",
    "Since some predictors are categorical in nature (i.e. nominal), it would make sense to convert these factor predictors into numeric dummy variables using step_dummy. To do this, the step selects all predictors then removes those that are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_vars <- imputed %>%\n",
    "        step_dummy(all_predictors(), -all_numeric()) \n",
    "ind_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Standardize___\n",
    "\n",
    "At this point in the recipe, all of the predictor should be encoded as numeric, we can further add more steps to center and scale them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized <- ind_vars %>%\n",
    "        step_center(all_predictors())  %>%\n",
    "        step_scale(all_predictors()) \n",
    "standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Prepare___\n",
    "\n",
    "We can now estimate the means and standard deviations from the training set. The prep function is used with a recipe and a data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rec <- prep(standardized, training = credit_train)\n",
    "trained_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Bake___\n",
    "\n",
    "Now that the statistics have been estimated, the preprocessing can be applied to the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data <- bake(trained_rec, new_data = credit_train) %>% na.omit()\n",
    "test_data  <- bake(trained_rec, new_data = credit_test)%>% na.omit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__The modeling phase__\n",
    "- Various modeling techniques are selected and applied\n",
    "- Often several techniques for the same problem\n",
    "- Besides the algorithms, we also need to specify evaluation procedure and metric\n",
    "- Specific requirements necessitate going back to preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Model Selection & Generalization__\n",
    "\n",
    "- Learning is an ill-posed problem: data is not sufficient to find a unique solution\n",
    "    - Supervised learning seeks to identify models that are able to make accurate predictions on unseen data that has similar characteristics as the data set used for training the model\n",
    "    - This is referred to as generalization and should guide model selection\n",
    "\n",
    "- To be able generalize a model needs to weigh complexity against observed error:\n",
    "    - More complex models have lower observed error on training data, might have higher true error (on unknown data)\n",
    "    - This is related to the bias-variance trade-off\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__The Bias-Variance Trade-off__\n",
    "\n",
    "\n",
    "<img src=\"http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png\" style=\"width:70%\">\n",
    " \n",
    "Source: http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Modeling with parsnip__\n",
    "\n",
    "The `parsnip` package is a unified interface to models. This should significantly reduce the amount of syntactical minutia that you’ll need to memorize by having one standardized model function across different packages and by harmonizing the parameter names across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(parsnip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Model specification___\n",
    "\n",
    "To use `parsnip`, you start with a model specification. This is a simple object that defines the intent of the model. We will be using a logistic regression for classifying the credit status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_model <- logistic_reg()\n",
    "credit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Model engine___\n",
    "\n",
    "`parsnip` offers a variety of methods to fit this general model. We differentiate these cases by the computational engines, which is a combination of the \n",
    "- Estimation type, such as least squares, and \n",
    "- Implemention \n",
    "    - R package or some other computing platform like Spark or Tensorflow\n",
    "    - `glm` is used to fit generalized linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_model %>%\n",
    "    set_engine(\"glm\") -> glm_credit_model\n",
    "glm_credit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Credit data: Fit model___\n",
    "\n",
    "Here are no additional arguments that we should specify here, so let’s jump to fitting the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_credit_model %>%\n",
    "  fit(Status ~ ., data = train_data) -> glm_fit\n",
    "glm_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Assess Performance with yardstick__\n",
    "\n",
    "`yardstick` is a package to estimate how well models are working using tidy data principle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predict(glm_fit, test_data)\n",
    "predicted_probs = predict(glm_fit, test_data, type = \"prob\")\n",
    "\n",
    "test_data %>%\n",
    "    select(Status) %>%\n",
    "    cbind(predicted, predicted_probs) -> res\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Confusion Matrix__\n",
    "\n",
    "In classification problems, the primary source for evaluation metrics is the confusion matrix.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*-BkpqhN-5fPicMifDQ0SwA.png\" style=\"width:40%\">\n",
    "\n",
    "\n",
    "see https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res %>%\n",
    "    conf_mat(Status, .pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Metrics for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy__\n",
    "\n",
    "The accuracy is the proportion of correct classifications. Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly).\n",
    "\n",
    "$ACC = \\frac{TP+TN}{TP+FP+FN+TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "res %>%\n",
    "    metrics(Status, .pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Precision__\n",
    "\n",
    "Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res %>%\n",
    "    precision(Status, .pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Recall__\n",
    "\n",
    "Recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.\n",
    "\n",
    "$Recall = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res %>%\n",
    "    recall(Status, .pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__ROC Curve__\n",
    "\n",
    "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res %>%\n",
    "    roc_curve(Status, .pred_bad) %>%\n",
    "    autoplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Up to you:  Titanic Passenger Survival Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Titanic Passenger Survival Data Set__\n",
    "\n",
    "This data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner \"Titanic\", summarized according to \n",
    "- economic status (class)\n",
    "- sex\n",
    "- age \n",
    "- survival \n",
    "\n",
    "The data sets are the individual non-aggregated observations and formatted in a machine learning context with a training sample, a testing sample, and two additional data sets that can be used for deeper machine learning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse(titanic::titanic_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Up to you: Titanic Passenger Survival__\n",
    "\n",
    "Build a classification model that predicts the survival or death of a passenger on the titanic, depending on the ticket class, age, sex, and port of embarkation.\n",
    "\n",
    "1. Split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Up to you: Titanic Passenger Survival__\n",
    "\n",
    "2. Prepare a recipe for data preprocessing (use ticket class, age, sex, and port of embarkation as variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Up to you: Titanic Passenger Survival__\n",
    "\n",
    "3. Bake train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Up to you: Titanic Passenger Survival__\n",
    "\n",
    "3. Fit the a logistic classification model (`glm`) on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Up to you: Titanic Passenger Survival__\n",
    "\n",
    "4. Evaluate the model on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='logo'><img src='images/uniwue4c.png'></div>",
   "scroll": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Agenda",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
