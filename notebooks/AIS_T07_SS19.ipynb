{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Analytical Information Systems*\n",
    "\n",
    "# Tutorial 7 - Unsupervised Learning\n",
    "\n",
    "Matthias Griebel<br>\n",
    "Lehrstuhl f√ºr Wirtschaftsinformatik und Informationsmanagement\n",
    "\n",
    "SS 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "toc": true
   },
   "source": [
    "<h1>Agenda<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Cluster-Analysis\" data-toc-modified-id=\"Cluster-Analysis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Cluster Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Partitional-clustering:-k-Means-Algorithm\" data-toc-modified-id=\"Partitional-clustering:-k-Means-Algorithm-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Partitional clustering: k-Means Algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Means-Clustering-in-R\" data-toc-modified-id=\"K-Means-Clustering-in-R-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>K-Means Clustering in R</a></span></li><li><span><a href=\"#Evaluating-K-means-Clusters\" data-toc-modified-id=\"Evaluating-K-means-Clusters-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Evaluating K-means Clusters</a></span></li></ul></li><li><span><a href=\"#Hierarchical-Clustering\" data-toc-modified-id=\"Hierarchical-Clustering-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Hierarchical Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hierarchical-Clustering-in-R\" data-toc-modified-id=\"Hierarchical-Clustering-in-R-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Hierarchical Clustering in R</a></span></li></ul></li></ul></li><li><span><a href=\"#Principal-Components-Analysis\" data-toc-modified-id=\"Principal-Components-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Principal Components Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#PCA-in-R\" data-toc-modified-id=\"PCA-in-R-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>PCA in R</a></span></li><li><span><a href=\"#Cluster-visualization-using-PCA\" data-toc-modified-id=\"Cluster-visualization-using-PCA-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Cluster visualization using PCA</a></span></li></ul></li><li><span><a href=\"#Exam-Questions\" data-toc-modified-id=\"Exam-Questions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exam Questions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exam-AIS-SS-2018\" data-toc-modified-id=\"Exam-AIS-SS-2018-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Exam AIS SS 2018</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Supervised learning__\n",
    "- Predict Y using X data\n",
    " \n",
    "__Unsupervised learning__\n",
    "- Only X features observed\n",
    "- Goal is to discover interesting things about the measurements\n",
    "- __Cluster Analysis__\n",
    "    - Can we discover subgroups among the variables or among the observations?\n",
    "- __Dimensionality Reduction__\n",
    "    - Can we improve information density of the data ‚Äì are there redundant variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__What is Cluster Analysis/Clustering?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- __Cluster__: a collection of data objects that are\n",
    "    - similar to one another within the same cluster\n",
    "    - dissimilar to the objects in other clusters\n",
    "\n",
    "\n",
    "- __Cluster analysis__\n",
    "    - grouping a set of data objects into clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src='images/07/cluster_analysis.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Types of Clusterings__\n",
    "\n",
    "- __Partitional Clustering__\n",
    "    - a division data objects into non-overlapping subsets (clusters) such that each data object is in exactly one subset\n",
    "- __Hierarchical clustering__\n",
    "    - a set of nested clusters organized as a hierarchical tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Load packages for today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "library(ggrepel)\n",
    "library(ggfortify)\n",
    "library(ggdendro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__The protein dataset__\n",
    "\n",
    "The data set contains data about 25 European countries and their protein intakes (in percent) from nine major food sources (p = 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://www.biz.uiowa.edu/faculty/jledolter/DataMining/protein.csv'\n",
    "food <- read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food %>% head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can we discover subgroups among the countries regarding their major food sources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=4)\n",
    "food %>%\n",
    "    ggplot(aes(RedMeat, WhiteMeat)) + \n",
    "    geom_point() + \n",
    "    geom_label_repel(aes(label=Country), size = 2) +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Partitional clustering: k-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "- Each cluster is associated with a centroid (center point)\n",
    "- Each point is assigned to the cluster with the closest centroid\n",
    "    - ‚ÄòCloseness‚Äô is measured by Euclidean distance, cosine similarity, correlation, etc.\n",
    "- Number of clusters, K, must be specified\n",
    "    - Initial centroids are often chosen randomly - clusters may vary from one run to another\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Objective:__ Minimize the within-cluster sum of squares (i.e. variance)\n",
    "$$\\underset{\\mathbf{C}}{arg\\,min} = \\sum_{i=1}^{K}\\sum_{x\\in C_i}dist^2(m_i,x)$$\n",
    "\n",
    "The basic k-means algorithm does not guarantee to find the optimum\n",
    "\n",
    "<img src='images/07/k-means.png' style=\"width:50%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### K-Means Clustering in R\n",
    "\n",
    "`kmeans()` performs a K-Means clustering on a data matrix.\n",
    "\n",
    "Important parameters:\n",
    "\n",
    "- `centers`:  Either the number of clusters, say ùëò, or a set of initial (distinct) cluster centroids\n",
    "- `iter.max`: The maximum number of iterations allowed.\n",
    "- `nstart`:  How many random initial centroid configurations should be chosen? Reports on the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, we start clustering on just Red and White meat with k=3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_meat <- food %>% select(RedMeat, WhiteMeat)\n",
    "km_meat <- kmeans(food_meat, centers = 3, nstart = 10)\n",
    "km_meat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's visualize the results\n",
    "- `augment` adds the clustering result to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=4)\n",
    "km_meat %>%\n",
    "    augment(food) %>%\n",
    "    ggplot(aes(x=RedMeat, y=WhiteMeat, color=.cluster)) +\n",
    "    geom_point() +\n",
    "    geom_label_repel(aes(label=Country), size = 3) +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Evaluating K-means Clusters\n",
    "\n",
    "- `between_SS`: The between-cluster sum of squares\n",
    "    -  measures heterogeneity between the clusters\n",
    "\n",
    "- `within_SS`: Vector of within-cluster sum of squares, one component per cluster.\n",
    "    -  measures homogeneity within the clusters  \n",
    "\n",
    "- `total_SS`= `between_SS` + $\\sum$ `within_SS`\n",
    "\n",
    "- Ratio `between_SS` / `total_SS`\n",
    "    - Properties of internal cohesion and external separation\n",
    "    - Ideally ratio should approach 1 (relatively small $\\sum$ `within_SS`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `glance` extracts a single-row summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glance(km_meat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- `tidy` summarizes on a per-cluster level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy(km_meat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's explore the effect of different choices of k, from 1 to 9, on this clustering.\n",
    "1. Cluster the data 9 times, each using a different value of k\n",
    "2. Create columns containing the tidied, glanced and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_clusts <- tibble(k = 1:9) %>%\n",
    "  mutate(\n",
    "    kclust = map(k, ~kmeans(food_meat, .x)),\n",
    "    tidied = map(kclust, tidy),\n",
    "    glanced = map(kclust, glance),\n",
    "    augmented = map(kclust, augment, food)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters <- k_clusts %>%\n",
    "  unnest(tidied)\n",
    "\n",
    "assignments <- k_clusts %>% \n",
    "  unnest(augmented)\n",
    "\n",
    "clusterings <- k_clusts %>%\n",
    "    unnest(glanced, .drop=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plot the different clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments %>%\n",
    "    ggplot(aes(RedMeat, WhiteMeat)) +\n",
    "    geom_point(aes(color = .cluster)) + \n",
    "    facet_wrap(~ k) +\n",
    "    geom_point(data = clusters, aes(x1, x2), size = 5, shape = \"x\") +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Elbow method to determine the optimal number of clusters for k-means clustering__\n",
    "- Idea: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data\n",
    "- There are many more metrics that try to define the optimal number of cluster (see `NbClust` package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterings %>%\n",
    "    ggplot(aes(x=k, y=tot.withinss)) + \n",
    "    geom_line() +\n",
    "    scale_x_continuous(breaks = c(1:9)) +\n",
    "    geom_vline(xintercept = 3, linetype = \"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we want to include all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_protein <- food %>%\n",
    "    select(-Country) %>%\n",
    "    kmeans(centers=3, nstart=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_protein %>% \n",
    "    augment(food) %>%\n",
    "    ggplot(aes(x=RedMeat, y=WhiteMeat, color=.cluster)) +\n",
    "    geom_point() +\n",
    "    geom_label_repel(aes(label=Country), size= 2) +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a better way to visualize the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "Produce a nested sequence of clusters, a tree, also called Dendrogram\n",
    "\n",
    "- __Agglomerative (bottom up) clustering__\n",
    "    - builds the dendrogram (tree) from the bottom level\n",
    "    - merges the most similar (or nearest) pair of clusters \n",
    "    - stops when all the data points are merged into a single cluster\n",
    "    \n",
    "- __Divisive (top down) clustering__\n",
    "    - starts with all data points in one cluster, the root\n",
    "    - splits the root into a set of child clusters\n",
    "    - stops when only singleton clusters of individual data points remain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hierarchical Clustering in R\n",
    "- `hclust()` performs an agglomerative hierarchical cluster analysis on a set of dissimilarities \n",
    "- `dist()` computes and returns the distance matrix by using the specified distance measure (i.e., euclidean diestance)\n",
    "- `ggdendrogram()` creates a dendrogram plot using `ggplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food %>%\n",
    "    column_to_rownames(\"Country\") %>%\n",
    "    dist(method = \"euclidean\") %>%\n",
    "    hclust(method = \"ward.D\") -> h_protein\n",
    "h_protein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plot the dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=4)\n",
    "h_protein  %>% \n",
    "    ggdendrogram(rotate = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Get clusters__\n",
    "\n",
    "`cutree` cuts a tree, e.g., as resulting from `hclust`, into several groups either by specifying the desired number(s) of groups or the cut height(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_results <- cutree(h_protein, k=3)\n",
    "h_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's again visualize the results. Is there a better way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=4)\n",
    "food %>%\n",
    "    mutate(h_clusters = as.factor(h_results)) %>%    \n",
    "    ggplot(aes(x=RedMeat, y=WhiteMeat, color=h_clusters)) +\n",
    "    geom_point() +\n",
    "    geom_label_repel(aes(label=Country), size= 2) +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Principal Components Analysis__\n",
    "\n",
    "__PCA__ produces a low-dimensional representation of a dataset\n",
    "- sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated\n",
    "\n",
    "We use __PCA__ for \n",
    " - data visualization\n",
    " - data pre-processing before supervised techniques are applied\n",
    "\n",
    "\n",
    "__PCA vs Clustering__\n",
    "- PCA looks for a low-dimensional representation of the observations that explains a good fraction of the variance\n",
    "- Clustering looks for homogeneous subgroups among the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  PCA in R\n",
    "\n",
    "`prcomp` performs a principal components analysis on the given data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food %>%\n",
    "    select(-Country) %>%\n",
    "    prcomp(center = T, scale. = T) -> pca_protein\n",
    "pca_protein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Scree-plot__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=4)\n",
    "screeplot(pca_protein)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather use `ggplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=4)\n",
    "tidy(pca_protein, \"pcs\") %>%\n",
    "    ggplot(aes(x=PC, y=std.dev^2)) + \n",
    "    geom_line() +\n",
    "    geom_point() + \n",
    "    scale_x_continuous(breaks = c(1:9)) +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster visualization using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's visualize the __K-Means__ results. Why is scaling the data so important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=7, repr.plot.height=7)\n",
    "pca_protein %>%\n",
    "        autoplot(x = 1,\n",
    "                 y = 2,\n",
    "                 data = data.frame(Cluster = as.factor(km_protein$cluster)),\n",
    "                 colour = \"Cluster\",\n",
    "                 loadings = TRUE,\n",
    "                 loadings.label = TRUE,\n",
    "                 loadings.colour = 'black',\n",
    "                 loadings.label.colour =  'black',\n",
    "                 loadings.label.repel = T,\n",
    "                 frame = TRUE) + \n",
    "    geom_label_repel(aes(label=food$Country), size = 3, alpha=0.5) +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's visualize the results of __hierarchical clustering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=7, repr.plot.height=7)\n",
    "pca_protein %>%\n",
    "        autoplot(x = 1,\n",
    "                 y = 2,\n",
    "                 data = data.frame(Cluster = as.factor(h_results)),\n",
    "                 colour = \"Cluster\",\n",
    "                 loadings = TRUE,\n",
    "                 loadings.label = TRUE,\n",
    "                 loadings.colour = 'black',\n",
    "                 loadings.label.colour =  'black',\n",
    "                 loadings.label.repel = T,\n",
    "                 frame = TRUE) + \n",
    "    geom_label_repel(aes(label=food$Country), size = 3, alpha=0.5) +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exam Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exam AIS SS 2018\n",
    "\n",
    "__Question 4: Unsupervised Learning__\n",
    "\n",
    "(c) __PCA__ You performed a principal component analysis (PCA) on a dataset of 200 military ships of the Second World War. The first four components are displayed in the following table\n",
    "\n",
    "\\begin{array}{l|rrrrl}\n",
    "\\hline\n",
    "                        & Components     \\\\ \\hline\n",
    "                        & 1          & 2     & 3     & 4     & ... \\\\ \\hline\n",
    "Water \\¬†displacement      & 0.95       & -0.09 & ‚àí0.12 & 0.22  & ... \\\\\n",
    "Length                  & 0.90       & 0.30  & ‚àí0.06 & ‚àí0.20 & ... \\\\\n",
    "Width                   & 0.97       & ‚àí0.12 & ‚àí0.03 & 0.03  & ... \\\\\n",
    "Engine \\ power            & 0.55       & 0.77  & ‚àí0.19 & ‚àí0.13 & ... \\\\\n",
    "Speed                   & ‚àí0.52      & 0.79  & ‚àí0.15 & 0.22  & ... \\\\\n",
    "Radius \\ of \\ action        & 0.39       & 0.31  & 0.86  & 0.03  & ... \\\\\n",
    "Crew \\¬†size               & 0.95       & 0.06  & ‚àí0.05 & 0.10  & ... \\\\ \\hline\n",
    "\\textbf{% of variance} & 64.88      & 19.22 & 10.43 & 2.22  & ... \\\\ \\hline\n",
    "\\end{array}\n",
    "\n",
    "i. (1 point) How many principal components are there in total? Briefly explain the number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(ii.) (2 points) A PCA is particularly helpful if it is possible to attribute a concept or interpretation to a component. Provide a meaningful explanation of the concepts embedded in principal component 1 and 2 of the ship data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "iii. (3 points) How would you proceed to use the PCA for dimensionality reduction purposes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='logo'><img src='images/uniwue4c.png'></div>",
   "scroll": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Agenda",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
